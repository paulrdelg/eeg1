{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdfbdb28-4f9a-43f9-a016-244e9526806e",
   "metadata": {},
   "source": [
    "## Project Final for ECE 556: AI for Radar and Remote Sensing\n",
    "Paul Delgado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f93738a-788b-4ee5-981c-089909ffc29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python packages\n",
    "import csv\n",
    "import datetime\n",
    "from itertools import product\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "import sys\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1d1318-00d2-44ec-8268-add786851e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party packages\n",
    "import mne_bids\n",
    "import numpy as np\n",
    "import openneuro\n",
    "import PIL.Image\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9832a0ee-4d79-44bc-9742-e16d06a03507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import caffe\n",
    "from scipy import optimize\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9ca94-1bd5-48c6-8d94-443e75fa02ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### I) Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f09c0e8-ae0e-485b-9acb-7bfab3a8d5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PaulDRP\\source\\repos\\eeg1\\results\n"
     ]
    }
   ],
   "source": [
    "# Data settings\n",
    "results_dirpath_str = os.path.normpath(os.path.join(os.getcwd(), 'results'))\n",
    "print(results_dirpath_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74a51205-d95b-4802-88ca-db60bbdfca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_list = ['S1', 'S2', 'S3']\n",
    "rois_list = ['VC']\n",
    "network_name_str = 'VGG19'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "252ed572-9daa-4450-837c-517929c7b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images in figure 2\n",
    "image_type = 'natural'\n",
    "image_label_list = ['Img0009', 'Img0002', 'Img0001', 'Img0005', 'Img0036', 'Img0045', 'Img0031', 'Img0043']\n",
    "max_iteration = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8f0af0e-7cc0-4329-b1ba-ec33fbca3a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PaulDRP\\source\\repos\\DeepImageReconstruction\n"
     ]
    }
   ],
   "source": [
    "# Data Directory\n",
    "dat_dirpath_str = os.path.normpath(os.path.join(os.getcwd(), '..', 'DeepImageReconstruction'))\n",
    "print(dat_dirpath_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d019618b-a542-47fc-8caf-d1e15e68ee24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PaulDRP\\source\\repos\\DeepImageReconstruction\\data\\decodedfeatures\n"
     ]
    }
   ],
   "source": [
    "decoded_features_dirpath_str = os.path.normpath(os.path.join(dat_dirpath_str, 'data', 'decodedfeatures'))\n",
    "print(decoded_features_dirpath_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7c6652a-81bf-4ad9-a3e6-e25005df6596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PaulDRP\\source\\repos\\DeepImageReconstruction\\data\\ilsvrc_2012_mean.npy\n"
     ]
    }
   ],
   "source": [
    "img_mean_fp = os.path.normpath(os.path.join(dat_dirpath_str, 'data', 'ilsvrc_2012_mean.npy'))\n",
    "print(img_mean_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a5e2f3d-e481-4423-a4e6-704a60a37b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PaulDRP\\source\\repos\\DeepImageReconstruction\\net\\VGG_ILSVRC_19_layers\\VGG_ILSVRC_19_layers.caffemodel\n",
      "C:\\Users\\PaulDRP\\source\\repos\\DeepImageReconstruction\\net\\VGG_ILSVRC_19_layers\\VGG_ILSVRC_19_layers.prototxt\n"
     ]
    }
   ],
   "source": [
    "# CNN model\n",
    "model_fp = os.path.normpath(os.path.join(dat_dirpath_str, 'net', 'VGG_ILSVRC_19_layers', 'VGG_ILSVRC_19_layers.caffemodel'))\n",
    "prototxt_fp = os.path.normpath(os.path.join(dat_dirpath_str, 'net', 'VGG_ILSVRC_19_layers', 'VGG_ILSVRC_19_layers.prototxt'))\n",
    "channel_swap = (2, 1, 0)\n",
    "print(model_fp)\n",
    "print(prototxt_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0882fe5e-61c0-424a-9ffa-baa81f103ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PaulDRP\\source\\repos\\DeepImageReconstruction\\data\\estimated_vgg19_cnn_feat_std.mat\n"
     ]
    }
   ],
   "source": [
    "feat_std_fp = os.path.normpath(os.path.join(dat_dirpath_str, 'data', 'estimated_vgg19_cnn_feat_std.mat'))\n",
    "print(feat_std_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8255bb3e-5b2f-48c0-8986-325812681d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ds001506'\n",
    "subject = '01'\n",
    "session = 'perceptionNaturalImageTraining01'\n",
    "run = 1\n",
    "#task = 'trance'\n",
    "#suffix = 'inplane'\n",
    "datatype = 'anat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ee856e4-5532-43d6-bcc9-9605e273c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bids_root_dirpath_str = os.path.normpath(os.path.join(os.getcwd(), '..', dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b69443b-0668-4716-afba-2ee5ac21a6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515c18a6-3c40-4187-b8c0-dc4c92571751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55a99139-d77a-4699-9903-8d46f07574af",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### II) Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0892b55c-f6c8-4d35-945f-a2e066db94a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_extreme_value(img, pct=1):\n",
    "    '''clip the pixels with extreme values'''\n",
    "    if pct < 0:\n",
    "        pct = 0.\n",
    "\n",
    "    if pct > 100:\n",
    "        pct = 100.\n",
    "\n",
    "    img = np.clip(img, np.percentile(img, pct/2.),\n",
    "                  np.percentile(img, 100-pct/2.))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "414cce90-176f-4eae-bb05-c54cf4524e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cnn_feat_std(cnn_feat):\n",
    "    feat_ndim = cnn_feat.ndim\n",
    "    feat_size = cnn_feat.shape\n",
    "    # for the case of fc layers\n",
    "    if feat_ndim == 1 or (feat_ndim == 2 and feat_size[0] == 1) or (feat_ndim == 3 and feat_size[1] == 1 and feat_size[2] == 1):\n",
    "        cnn_feat_std = np.std(cnn_feat)\n",
    "    # for the case of conv layers\n",
    "    elif feat_ndim == 3 and (feat_size[1] > 1 or feat_size[2] > 1):\n",
    "        num_of_ch = feat_size[0]\n",
    "        # std for each channel\n",
    "        cnn_feat_std = np.zeros(num_of_ch, dtype='float32')\n",
    "        for j in range(num_of_ch):\n",
    "            feat_ch = cnn_feat[j, :, :]\n",
    "            cnn_feat_std[j] = np.std(feat_ch)\n",
    "        cnn_feat_std = np.mean(cnn_feat_std)  # std averaged across channels\n",
    "    return cnn_feat_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fef17ca-70fd-4c01-84c9-ec3b5d7d4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_loss(feat, feat0, mask=1.):\n",
    "    d = feat - feat0\n",
    "    loss = (d*d*mask).sum()\n",
    "    grad = 2 * d * mask\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f9f9b2f-15b5-42ba-9121-acca9daa9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_loss_fun(loss_type):\n",
    "    if loss_type == 'l2':\n",
    "        return L2_loss\n",
    "    elif loss_type == 'l1':\n",
    "        return L1_loss\n",
    "    elif loss_type == 'inner':\n",
    "        return inner_loss\n",
    "    elif loss_type == 'gram':\n",
    "        return gram_loss\n",
    "    else:\n",
    "        raise ValueError('unknown loss function type!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54c270cf-2666-463d-8970-fbe1ed2df79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path_to_img):\n",
    "    max_dim = 512\n",
    "    img = tf.io.read_file(path_to_img)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    \n",
    "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "    long_dim = max(shape)\n",
    "    scale = max_dim / long_dim\n",
    "    \n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "    \n",
    "    img = tf.image.resize(img, new_shape)\n",
    "    img = img[tf.newaxis, :]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93eff75d-622e-4893-aca8-31e0dc8eb89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_masks(features, masks=None, channels=None):\n",
    "    feature_masks = {}\n",
    "    for layer in features.keys():\n",
    "        if (masks is None or masks == {} or masks == [] or (layer not in masks.keys())) and (channels is None or channels == {} or channels == [] or (layer not in channels.keys())):  # use all features and all channels\n",
    "            feature_masks[layer] = np.ones_like(features[layer])\n",
    "        elif isinstance(masks, dict) and (layer in masks.keys()) and isinstance(masks[layer], np.ndarray) and masks[layer].ndim == 3 and masks[layer].shape[0] == features[layer].shape[0] and masks[layer].shape[1] == features[layer].shape[1] and masks[layer].shape[2] == features[layer].shape[2]:  # 3D mask\n",
    "            feature_masks[layer] = masks[layer]\n",
    "        # 1D feat and 1D mask\n",
    "        elif isinstance(masks, dict) and (layer in masks.keys()) and isinstance(masks[layer], np.ndarray) and features[layer].ndim == 1 and masks[layer].ndim == 1 and masks[layer].shape[0] == features[layer].shape[0]:\n",
    "            feature_masks[layer] = masks[layer]\n",
    "        elif (masks is None or masks == {} or masks == [] or (layer not in masks.keys())) and isinstance(channels, dict) and (layer in channels.keys()) and isinstance(channels[layer], np.ndarray) and channels[layer].size > 0:  # select channels\n",
    "            mask_2D = np.ones_like(features[layer][0])\n",
    "            mask_3D = np.tile(mask_2D, [len(channels[layer]), 1, 1])\n",
    "            feature_masks[layer] = np.zeros_like(features[layer])\n",
    "            feature_masks[layer][channels[layer], :, :] = mask_3D\n",
    "        # use 2D mask select features for all channels\n",
    "        elif isinstance(masks, dict) and (layer in masks.keys()) and isinstance(masks[layer], np.ndarray) and masks[layer].ndim == 2 and (channels is None or channels == {} or channels == [] or (layer not in channels.keys())):\n",
    "            mask_2D_0 = masks[layer]\n",
    "            mask_size0 = mask_2D_0.shape\n",
    "            mask_size = features[layer].shape[1:]\n",
    "            if mask_size0[0] == mask_size[0] and mask_size0[1] == mask_size[1]:\n",
    "                mask_2D = mask_2D_0\n",
    "            else:\n",
    "                mask_2D = np.ones(mask_size)\n",
    "                n_dim1 = min(mask_size0[0], mask_size[0])\n",
    "                n_dim2 = min(mask_size0[1], mask_size[1])\n",
    "                idx0_dim1 = np.arange(n_dim1) + \\\n",
    "                    round((mask_size0[0] - n_dim1)/2)\n",
    "                idx0_dim2 = np.arange(n_dim2) + \\\n",
    "                    round((mask_size0[1] - n_dim2)/2)\n",
    "                idx_dim1 = np.arange(n_dim1) + round((mask_size[0] - n_dim1)/2)\n",
    "                idx_dim2 = np.arange(n_dim2) + round((mask_size[1] - n_dim2)/2)\n",
    "                mask_2D[idx_dim1, idx_dim2] = mask_2D_0[idx0_dim1, idx0_dim2]\n",
    "            feature_masks[layer] = np.tile(\n",
    "                mask_2D, [features[layer].shape[0], 1, 1])\n",
    "        else:\n",
    "            feature_masks[layer] = 0\n",
    "\n",
    "    return feature_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9bbcf8e1-47e7-4ca5-9ccd-a1ae210a5456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_deprocess(img, img_mean=np.float32([104, 117, 123])):\n",
    "    '''convert from Caffe's input image layout'''\n",
    "    return np.dstack((img + np.reshape(img_mean, (3, 1, 1)))[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5788358-2027-40bc-b34e-d3fc952f838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fun(img, net, features, feature_masks, layer_weight, loss_fun, save_intermediate, save_intermediate_every, save_intermediate_path, save_intermediate_ext, save_intermediate_postprocess, loss_list=[]):\n",
    "    # reshape img\n",
    "    img_size = net.blobs['data'].data.shape[-3:]\n",
    "    img = img.reshape(img_size)\n",
    "\n",
    "    # save intermediate image\n",
    "    t = len(loss_list)\n",
    "    if save_intermediate and (t % save_intermediate_every == 0):\n",
    "        img_mean = net.transformer.mean['data']\n",
    "        save_path = os.path.join(save_intermediate_path, '%05d.%s' % (t, save_intermediate_ext))\n",
    "        if save_intermediate_postprocess is None:\n",
    "            snapshot_img = img_deprocess(img, img_mean)\n",
    "        else:\n",
    "            snapshot_img = save_intermediate_postprocess(img_deprocess(img, img_mean))\n",
    "        PIL.Image.fromarray(snapshot_img).save(save_path)\n",
    "\n",
    "    # layer_list\n",
    "    layer_list = features.keys()\n",
    "    layer_list = sort_layer_list(net, layer_list)\n",
    "\n",
    "    # num_of_layer\n",
    "    num_of_layer = len(layer_list)\n",
    "\n",
    "    # cnn forward\n",
    "    net.blobs['data'].data[0] = img.copy()\n",
    "    net.forward(end=layer_list[-1])\n",
    "\n",
    "    # cnn backward\n",
    "    loss = 0.\n",
    "    layer_start = layer_list[-1]\n",
    "    net.blobs[layer_start].diff.fill(0.)\n",
    "    for j in xrange(num_of_layer):\n",
    "        layer_start_index = num_of_layer - 1 - j\n",
    "        layer_end_index = num_of_layer - 1 - j - 1\n",
    "        layer_start = layer_list[layer_start_index]\n",
    "        if layer_end_index >= 0:\n",
    "            layer_end = layer_list[layer_end_index]\n",
    "        else:\n",
    "            layer_end = 'data'\n",
    "        feat_j = net.blobs[layer_start].data[0].copy()\n",
    "        feat0_j = features[layer_start]\n",
    "        mask_j = feature_masks[layer_start]\n",
    "        layer_weight_j = layer_weight[layer_start]\n",
    "        loss_j, grad_j = loss_fun(feat_j, feat0_j, mask_j)\n",
    "        loss_j = layer_weight_j * loss_j\n",
    "        grad_j = layer_weight_j * grad_j\n",
    "        loss = loss + loss_j\n",
    "        g = net.blobs[layer_start].diff[0].copy()\n",
    "        g = g + grad_j\n",
    "        net.blobs[layer_start].diff[0] = g.copy()\n",
    "        if layer_end == 'data':\n",
    "            net.backward(start=layer_start)\n",
    "        else:\n",
    "            net.backward(start=layer_start, end=layer_end)\n",
    "        net.blobs[layer_start].diff.fill(0.)\n",
    "    grad = net.blobs['data'].diff[0].copy()\n",
    "\n",
    "    # reshape gradient\n",
    "    grad = grad.flatten().astype(np.float64)\n",
    "    loss_list.append(loss)\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b8d8651-61a7-497d-9d9d-a7c7148aeb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_image(features, net,\n",
    "                      layer_weight=None, \n",
    "                      channel=None, \n",
    "                      mask=None, \n",
    "                      initial_image=None, \n",
    "                      loss_type='l2', \n",
    "                      maxiter=500, \n",
    "                      disp=True, \n",
    "                      save_intermediate=False, \n",
    "                      save_intermediate_every=1, \n",
    "                      save_intermediate_path=None,\n",
    "                      save_intermediate_ext='jpg'):\n",
    "    # loss function\n",
    "    #loss_fun = switch_loss_fun(loss_type)\n",
    "    loss_fun = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    # make dir for saving intermediate\n",
    "    if save_intermediate:\n",
    "        if save_intermediate_path is None:\n",
    "            fn_temp1 = datetime.datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "            fn_name = 'recon_img_lbfgs_snapshots' + fn_temp1\n",
    "            fp_relative = os.path.join(os.getcwd(), fn_name)\n",
    "            save_intermediate_path = os.path.normpath(fp_relative)\n",
    "        if not os.path.exists(save_intermediate_path):\n",
    "            os.makedirs(save_intermediate_path)\n",
    "\n",
    "    # image size\n",
    "    #img_size = net.blobs['data'].data.shape[-3:]\n",
    "    img_size = net.layers[0].shape[-3:]\n",
    "    print(f'image size: {img_size}')\n",
    "\n",
    "    # num of pixel\n",
    "    num_of_pix = np.prod(img_size)\n",
    "    print(f'num of pix: {num_of_pix}')\n",
    "\n",
    "    # image mean\n",
    "    #img_mean = net.transformer.mean['data']\n",
    "    img_mean_fp = os.path.normpath(os.path.join(dat_dirpath_str, 'data', 'ilsvrc_2012_mean.npy'))\n",
    "    img_mean = np.load(img_mean_fp)\n",
    "    print(type(img_mean))\n",
    "    img_mean = np.float32([img_mean[0].mean(), img_mean[1].mean(), img_mean[2].mean()])\n",
    "    print(f'image mean: {img_mean.shape}')\n",
    "\n",
    "    # img bounds\n",
    "    img_min = -img_mean\n",
    "    print(f'image min: {img_min.shape}')\n",
    "    \n",
    "    img_max = img_min + 255.\n",
    "    \n",
    "    pix_temp = num_of_pix / 3\n",
    "    \n",
    "    img_min0_tuple = (img_min[0], img_max[0])\n",
    "    img_min0_list = [img_min0_tuple]\n",
    "    img_min0 = np.array(img_min0_list) * pix_temp\n",
    "    \n",
    "    img_min1_tuple = (img_min[1], img_max[1])\n",
    "    img_min1_list = [img_min1_tuple]\n",
    "    img_min1 = np.array(img_min1_list) * pix_temp\n",
    "    \n",
    "    img_min2_tuple = (img_min[2], img_max[2])\n",
    "    img_min2_list = [img_min2_tuple]\n",
    "    img_min2 = np.array(img_min2_list) * pix_temp\n",
    "    \n",
    "    img_bounds = img_min0 + img_min1 + img_min2\n",
    "    \n",
    "    # initial image\n",
    "    if initial_image is None:\n",
    "        initial_image = np.random.randint(0, 256, (img_size[1], img_size[2], img_size[0]))\n",
    "    if save_intermediate:\n",
    "        save_name = 'initial_img.png'\n",
    "        fp = os.path.join(save_intermediate_path, save_name)\n",
    "        PIL.Image.fromarray(np.uint8(initial_image)).save(fp)\n",
    "\n",
    "    # preprocess initial img\n",
    "    #initial_image = img_preprocess(initial_image, img_mean)\n",
    "    initial_image = tf.keras.applications.vgg19.preprocess_input(initial_image * img_mean)\n",
    "    initial_image = initial_image.flatten()\n",
    "\n",
    "    # layer_list\n",
    "    layer_list = features.keys()\n",
    "    print(layer_list)\n",
    "    #layer_list = sort_layer_list(net, layer_list) # PD: already in order\n",
    "\n",
    "    # number of layers\n",
    "    num_of_layer = len(layer_list)\n",
    "\n",
    "    # layer weight\n",
    "    if layer_weight is None:\n",
    "        weights = np.ones(num_of_layer)\n",
    "        weights = np.float32(weights)\n",
    "        weights = weights / weights.sum()\n",
    "        layer_weight = {}\n",
    "        for j, layer in enumerate(layer_list):\n",
    "            layer_weight[layer] = weights[j]\n",
    "\n",
    "    # feature mask\n",
    "    feature_masks = create_feature_masks(features, masks=mask, channels=channel)\n",
    "\n",
    "    # optimization params\n",
    "    loss_list = []\n",
    "    opt_params = {\n",
    "        'args': (net, features, feature_masks, layer_weight, loss_fun, save_intermediate, save_intermediate_every, save_intermediate_path, save_intermediate_ext, loss_list),\n",
    "        'method': 'L-BFGS-B',\n",
    "        'jac': True,\n",
    "        'bounds': img_bounds,\n",
    "        'options': {'maxiter': maxiter, 'disp': disp},\n",
    "    }\n",
    "    \n",
    "    print(type(features))\n",
    "    \n",
    "    # optimization\n",
    "    #res = optimize.minimize(obj_fun, initial_image, **opt_params)\n",
    "    opt = tf.keras.optimizers.SGD()\n",
    "    #opt_op = opt.minimize(loss_fun)\n",
    "    #opt_op.run()\n",
    "\n",
    "    # recon img\n",
    "    #img = res.x\n",
    "    #img = net(\n",
    "    img = initial_image.reshape(img_size)\n",
    "\n",
    "    # return img\n",
    "    return img_deprocess(img, img_mean), loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3487bd-428d-4c9e-908f-6ca43fd38462",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### III) Load Input Files & Create Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e52c9cb-b8ce-4826-a2e8-03f72292b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mean = np.load(img_mean_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "963afc76-db86-433a-8b38-e23637de4d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.vgg19.VGG19(include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c45ba1da-c39a-4d17-95da-bf041e4af05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature SD estimated from true CNN features of 10000 images\n",
    "feat_std0 = sio.loadmat(feat_std_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6a52c1e-c5fd-4cfe-85a4-020112d7fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(bids_root_dirpath_str):\n",
    "    os.makedirs(bids_root_dirpath_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c371d478-07ff-4562-9d6d-353be0205338",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(results_dirpath_str):\n",
    "    os.makedirs(results_dirpath_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d39aa-59e4-497e-a6e3-34abf9a33333",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### IV) Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc4168d2-e89f-4f4b-85f8-b0145bf04727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data (BIDS-standard repo)\n",
    "#openneuro.download(dataset=dataset, target_dir=bids_root, include=[f'sub-{subject}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5f8b6eb-a24d-468d-a4d4-b8c2c67e243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bids_path = mne_bids.BIDSPath(root=bids_root_dirpath_str, session=session, datatype=datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f96c3e-f6de-44f1-b268-6c694a876203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "269cde46-58e7-4a2f-88ca-c300922960ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### V) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d434ed2-5d83-4aa5-aaec-0a9371504227",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Suffix anat is not allowed. Use one of these suffixes ['meg', 'markers', 'eeg', 'ieeg', 'T1w', 'FLASH', 'participants', 'scans', 'electrodes', 'optodes', 'channels', 'coordsystem', 'events', 'headshape', 'digitizer', 'beh', 'physio', 'stim', 'nirs'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m raw \u001b[38;5;241m=\u001b[39m \u001b[43mmne_bids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_raw_bids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbids_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbids_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<decorator-gen-579>:12\u001b[0m, in \u001b[0;36mread_raw_bids\u001b[1;34m(bids_path, extra_params, verbose)\u001b[0m\n",
      "File \u001b[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\mne_bids\\read.py:654\u001b[0m, in \u001b[0;36mread_raw_bids\u001b[1;34m(bids_path, extra_params, verbose)\u001b[0m\n\u001b[0;32m    652\u001b[0m     bids_path\u001b[38;5;241m.\u001b[39mupdate(datatype\u001b[38;5;241m=\u001b[39mdatatype)\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 654\u001b[0m     \u001b[43mbids_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bids_path\u001b[38;5;241m.\u001b[39mfpath\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    657\u001b[0m     bids_raw_folder \u001b[38;5;241m=\u001b[39m bids_path\u001b[38;5;241m.\u001b[39mdirectory \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbids_path\u001b[38;5;241m.\u001b[39mbasename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\mne_bids\\path.py:749\u001b[0m, in \u001b[0;36mBIDSPath.update\u001b[1;34m(self, check, **kwargs)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mold_kwargs)\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck \u001b[38;5;241m=\u001b[39m old_check\n\u001b[1;32m--> 749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\mne_bids\\path.py:743\u001b[0m, in \u001b[0;36mBIDSPath.update\u001b[1;34m(self, check, **kwargs)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;66;03m# Perform a check of the entities and revert changes if check fails\u001b[39;00m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 743\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    745\u001b[0m     old_check \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck\n",
      "File \u001b[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\mne_bids\\path.py:867\u001b[0m, in \u001b[0;36mBIDSPath._check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    864\u001b[0m suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuffix\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    866\u001b[0m         suffix \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ALLOWED_FILENAME_SUFFIX:\n\u001b[1;32m--> 867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSuffix \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not allowed. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    868\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUse one of these suffixes \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    869\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mALLOWED_FILENAME_SUFFIX\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Suffix anat is not allowed. Use one of these suffixes ['meg', 'markers', 'eeg', 'ieeg', 'T1w', 'FLASH', 'participants', 'scans', 'electrodes', 'optodes', 'channels', 'coordsystem', 'events', 'headshape', 'digitizer', 'beh', 'physio', 'stim', 'nirs']."
     ]
    }
   ],
   "source": [
    "raw = mne_bids.read_raw_bids(bids_path=bids_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c814365c-5d49-4814-a0b2-60da00ac00ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw.info['subject_info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3730a-ff56-45d0-aefd-e8f3b36f4528",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw.info['sfreq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e76fa0c-4ae3-48f8-9c9d-a8507bc0ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0238970-d79e-4f0f-8f56-ade2bf67fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw.annotations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1691c5e-0b11-4794-9338-1ebf6897d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw.annotations[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4f2f7-9440-479f-bc88-2c2ddb468088",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw.annotations[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a337be54-9aab-4007-a11b-393102522203",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f328977-9792-4e18-857a-870292b3052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_bids.inspect_dataset(bids_root_dirpath_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c729ad53-74a7-4745-8612-b1347b4fd7dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### VI) Format BIDS into Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f169a68-92b4-4a16-b65a-1fedf3e8247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef8534d-0ed9-421b-b80b-54c47270f065",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### VII) Initialize Given Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "113c42a9-a12b-4a88-97a7-733e0d3ceaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104.00699 116.66877 122.67892]\n"
     ]
    }
   ],
   "source": [
    "# Average image of ImageNet\n",
    "img_mean_ndarray = np.float32([img_mean[0].mean(), img_mean[1].mean(), img_mean[2].mean()])\n",
    "print(img_mean_ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71fee6bd-575e-4c63-8f13-1c9672ce5fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial input shape: (None, 224, 224, 3)\n",
      "height: 224 and width: 224\n",
      "model reshaped: (None, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "#net = caffe.Classifier(prototxt_file, model_file, mean=img_mean, channel_swap=channel_swap)\n",
    "print(f'initial input shape: {model.input.shape}')\n",
    "model.layers[0].shape = (10, 3, 224, 224) #in1 = tf.keras.layers.Reshape((10, 3, 224, 224))\n",
    "\n",
    "#h, w = net.blobs['data'].data.shape[-2:]\n",
    "h, w = model.input.shape[1:3]\n",
    "print(f'height: {h} and width: {w}')\n",
    "\n",
    "#net.blobs['data'].reshape(1, 3, h, w)\n",
    "model.layers[0].shape = (1, 3, h, w)\n",
    "print(f'model reshaped: {model.input.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28c492b3-455c-4b61-9c5c-9be696fb16f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial image shape: (224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Initial image for the optimization (here we use the mean of ilsvrc_2012_mean.npy as RGB values)\n",
    "img_init_ndarray = np.zeros((h, w, 3), dtype='float32')\n",
    "img_init_ndarray[:, :, 0] = img_mean_ndarray[2].copy()\n",
    "img_init_ndarray[:, :, 1] = img_mean_ndarray[1].copy()\n",
    "img_init_ndarray[:, :, 2] = img_mean_ndarray[0].copy()\n",
    "print(f'initial image shape: {img_init_ndarray.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5711be66-283f-40cf-9535-61c13efcf91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0: input_1\n",
      "layer 1: block1_conv1 is conv1_1\n",
      "layer 2: block1_conv2 is conv1_2\n",
      "layer 3: block1_pool is pool1\n",
      "layer 4: block2_conv1 is conv2_1\n",
      "layer 5: block2_conv2 is conv2_2\n",
      "layer 6: block2_pool is pool2\n",
      "layer 7: block3_conv1 is conv3_1\n",
      "layer 8: block3_conv2 is conv3_2\n",
      "layer 9: block3_conv3 is conv3_3\n",
      "layer 10: block3_conv4 is conv3_4\n",
      "layer 11: block3_pool is pool3\n",
      "layer 12: block4_conv1 is conv4_1\n",
      "layer 13: block4_conv2 is conv4_2\n",
      "layer 14: block4_conv3 is conv4_3\n",
      "layer 15: block4_conv4 is conv4_4\n",
      "layer 16: block4_pool is pool4\n",
      "layer 17: block5_conv1 is conv5_1\n",
      "layer 18: block5_conv2 is conv5_2\n",
      "layer 19: block5_conv3 is conv5_3\n",
      "layer 20: block5_conv4 is conv5_4\n",
      "layer 21: block5_pool is pool5\n",
      "layer 22: Flatten\n",
      "layer 23: fc1 is fc6\n",
      "layer 24: fc2 is fc7\n",
      "layer 25: predictions is fc8\n"
     ]
    }
   ],
   "source": [
    "# CNN Layers (all conv and fc layers)\n",
    "layers = []\n",
    "count = 0\n",
    "pool_count = 1\n",
    "block_count = 1\n",
    "for layer in model.layers:\n",
    "    if layer.__class__.__name__ == 'InputLayer':\n",
    "        print(f'layer {count}: {layer.name}')\n",
    "    elif layer.__class__.__name__ == 'Conv2D':\n",
    "        sections = layer.name.split('_')\n",
    "        block = sections[0]\n",
    "        blockNum = block[-1]\n",
    "        if int(blockNum) > block_count:\n",
    "            block_count = int(blockNum)\n",
    "        layerNum = sections[1][-1]\n",
    "        val = 'conv' + blockNum + '_' + layerNum\n",
    "        print(f'layer {count}: {layer.name} is {val}')\n",
    "        layers.append(val)\n",
    "    elif layer.__class__.__name__ == 'MaxPooling2D':\n",
    "        sections = layer.name.split('_')\n",
    "        block = sections[0]\n",
    "        blockNum = block[-1]\n",
    "        if int(blockNum) > block_count:\n",
    "            block_count = int(blockNum)\n",
    "        val = 'pool' + blockNum\n",
    "        print(f'layer {count}: {layer.name} is {val}')\n",
    "        layers.append(val)\n",
    "        pool_count = pool_count + 1\n",
    "    elif layer.__class__.__name__ == 'Flatten':\n",
    "        print(f'layer {count}: Flatten')\n",
    "    elif layer.__class__.__name__ == 'Dense':\n",
    "        block_count = block_count + 1\n",
    "        val = 'fc' + str(block_count)\n",
    "        print(f'layer {count}: {layer.name} is {val}')\n",
    "        layers.append(val)\n",
    "    else:\n",
    "        print(f'Error finding layer {count}: {layer.name}')\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf4c3c46-0e22-4671-8c07-198ec02c4d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_type': 'l2', 'maxiter': 200, 'initial_image': None, 'disp': True}\n"
     ]
    }
   ],
   "source": [
    "opts_dict = {\n",
    "    'loss_type': 'l2', # The loss function type: {'l2','l1','inner','gram'}\n",
    "    'maxiter': max_iteration, # The maximum number of iterations\n",
    "    'initial_image': None, # (setting to None will use random noise as initial image)\n",
    "    'disp': True # Display the information on the terminal or not\n",
    "}\n",
    "print(opts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8b28a1-704c-42d7-99b6-c874f0d55b9a",
   "metadata": {},
   "source": [
    "#### VII) Main Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5296552-9ecf-44d1-9ad1-9d85e22e5717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss_type\": \"l2\", \"maxiter\": 200, \"initial_image\": null, \"disp\": true}\n"
     ]
    }
   ],
   "source": [
    "# Save the optional parameters\n",
    "opts_json_str = json.dumps(opts_dict)\n",
    "print(opts_json_str)\n",
    "opts_fn = os.path.normpath(os.path.join(results_dirpath_str, 'options.json'))\n",
    "options_file_handler = open(opts_fn, 'w')\n",
    "options_file_handler.write(obj_json_str)\n",
    "options_file_handler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a15eb4b-438a-4638-839d-0de5a2042e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject:     S1\n",
      "ROI:         VC\n",
      "Image label: Img0009\n",
      "\n",
      "save dir: C:\\Users\\PaulDRP\\source\\repos\\eeg1\\results\\S1\\VC\n",
      "loading decoded CNN features\n",
      "image size: (3, 224, 224)\n",
      "num of pix: 150528\n",
      "<class 'numpy.ndarray'>\n",
      "image mean: (3,)\n",
      "image min: (3,)\n",
      "dict_keys(['conv1_1', 'conv1_2', 'pool1', 'conv2_1', 'conv2_2', 'pool2', 'conv3_1', 'conv3_2', 'conv3_3', 'conv3_4', 'pool3', 'conv4_1', 'conv4_2', 'conv4_3', 'conv4_4', 'pool4', 'conv5_1', 'conv5_2', 'conv5_3', 'conv5_4', 'pool5', 'fc6', 'fc7', 'fc8'])\n",
      "<class 'dict'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute '__array_interface__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m recon_img \u001b[38;5;241m=\u001b[39m clip_extreme_value(recon_img, pct\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     63\u001b[0m img_np \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mper_image_standardization(recon_img)\n\u001b[1;32m---> 64\u001b[0m \u001b[43mPIL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_np\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msave(save_fp)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\PIL\\Image.py:2803\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   2764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfromarray\u001b[39m(obj, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2765\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2766\u001b[0m \u001b[38;5;124;03m    Creates an image memory from an object exporting the array interface\u001b[39;00m\n\u001b[0;32m   2767\u001b[0m \u001b[38;5;124;03m    (using the buffer protocol).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2801\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.1.6\u001b[39;00m\n\u001b[0;32m   2802\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2803\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__array_interface__\u001b[49m\n\u001b[0;32m   2804\u001b[0m     shape \u001b[38;5;241m=\u001b[39m arr[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2805\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(shape)\n",
      "File \u001b[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:513\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mravel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    506\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtolist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    507\u001b[0m   \u001b[38;5;66;03m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[0;32m    508\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;124m    np_config.enable_numpy_behavior()\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n\u001b[1;32m--> 513\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '__array_interface__'"
     ]
    }
   ],
   "source": [
    "# Reconstrucion\n",
    "for subject, roi, image_label in product(subjects_list, rois_list, image_label_list):\n",
    "    print(f'Subject:     {subject}')\n",
    "    print(f'ROI:         {roi}')\n",
    "    print(f'Image label: {image_label}')\n",
    "    print('')\n",
    "\n",
    "    save_dir = os.path.join(results_dirpath_str, subject, roi)\n",
    "    print(f'save dir: {save_dir}')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Load the decoded CNN features\n",
    "    features = {}\n",
    "    print('loading decoded CNN features')\n",
    "    for layer in layers:\n",
    "        # The file full name depends on the data structure for decoded CNN features\n",
    "        name = f'{image_type}-{network_name_str}-{layer}-{subject}-{roi}-{image_label}.mat'\n",
    "        file_name = os.path.join(decoded_features_dirpath_str, image_type, network_name_str, layer, subject, roi, name)\n",
    "        feat = sio.loadmat(file_name)['feat']\n",
    "        if 'fc' in layer:\n",
    "            feat = feat.reshape(feat.size)\n",
    "\n",
    "        # Correct the norm of the decoded CNN features\n",
    "        feat_std = estimate_cnn_feat_std(feat)\n",
    "        feat = (feat / feat_std) * feat_std0[layer]\n",
    "        features.update({layer: feat})\n",
    "\n",
    "    # Weight of each layer in the total loss function\n",
    "\n",
    "    # Norm of the CNN features for each layer\n",
    "    feat_norm = np.array([np.linalg.norm(features[layer]) for layer in layers], dtype='float32')\n",
    "\n",
    "    # Use the inverse of the squared norm of the CNN features as the weight for each layer\n",
    "    weights = 1. / (feat_norm ** 2)\n",
    "\n",
    "    # Normalise the weights such that the sum of the weights = 1\n",
    "    weights = weights / weights.sum()\n",
    "    layer_weight = dict(zip(layers, weights))\n",
    "\n",
    "    opts_dict.update({'layer_weight': layer_weight})\n",
    "\n",
    "    # Reconstruction\n",
    "    snapshots_dir = os.path.join(save_dir, 'snapshots', f'image-{image_label}')\n",
    "    recon_img, loss_list = reconstruct_image(features,\n",
    "                                             model,\n",
    "                                             save_intermediate=True,\n",
    "                                             save_intermediate_path=snapshots_dir,\n",
    "                                             **opts_dict)\n",
    "\n",
    "    # Save the results\n",
    "\n",
    "    # Save the raw reconstructed image\n",
    "    save_name = 'recon_img' + '-' + image_label + '.mat'\n",
    "    sio.savemat(os.path.join(save_dir, save_name), {'recon_img': recon_img})\n",
    "\n",
    "    # To better display the image, clip pixels with extreme values (0.02% of pixels with extreme low values\n",
    "    # and 0.02% of the pixels with extreme high values). And then normalise the image by mapping\n",
    "    # the pixel value to be within [0,255].\n",
    "    save_name = 'recon_img_normalized' + '-' + image_label + '.jpg'\n",
    "    save_fp = os.path.join(save_dir, save_name)\n",
    "    recon_img = clip_extreme_value(recon_img, pct=4)\n",
    "    img_np = tf.image.per_image_standardization(recon_img)\n",
    "    PIL.Image.fromarray(img_np).save(save_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eb03e3-85b4-4bfd-a0cf-016ed11333d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1612aa-8f2a-46d9-951c-316da2035a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872c37fb-2dc5-4f9b-88ae-7938f0cec914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f12ec-e9a8-40e8-bfe2-faa4a277a627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1339603-4428-41bf-9ab7-96efbe1b6d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06ee7812-caeb-4718-b8b3-4769af91af29",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad01c8-b519-43d8-b0e6-a6f1006b0de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()\n",
    "#model.compile(loss=tf.keras.losses.MeanAbsoluteError(), optimizer=tf.keras.optimizers.Adam(), metrics=tf.keras.metrics.Accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b161af7-2a89-471c-82bc-05e16d12a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04ee339-1654-467e-8f96-041cca799710",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5875b8ca-0093-45f5-ab71-5188d6aa938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a12ef-80ef-49b7-a804-8754815d0bab",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f26ad9a-b173-43bd-b955-257bff8f1924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss, accuracy = model.evaluate(, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d15a09-be64-4d68-bfbb-e49c5a7fe2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
